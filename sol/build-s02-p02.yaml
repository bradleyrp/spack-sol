spack:
  # CUSTOM MODULES
  # these modifications blacklist the system compiler to make Core packages
  # this streamlines the lmod tree that we expose to the users
  # dev: could have sworn you could not set this, but now it is required or else
  #   we have collisions
  view: false
  concretizer: 
    # note that things were unified without asking when we ask for two very
    #   similar openmpi packages, but we could probably switch to when_possible
    unify: false
    reuse: false
  modules:
    # configurations for a module set
    default:
      roots:
        lmod: ./lmod
      arch_folder: false
      lmod:
        core_compilers:
        - &gcc_back !system_compiler
        hierarchy:
        - compiler
        - mpi
        exclude:
        - !cat ['%', *gcc_back]
        include:
        - gcc
        - openmpi
        - mvapich2
        - intel-oneapi-compilers
        exclude_implicits: true
        projections:
          all: '{name}/{version}'
          'openmpi fabrics=none': '{name}-intra/{version}'
        hash_length: 0
  # dev: try match to external gcc via: spack compiler find --scope env:build <path>
  #   however pasting that in here does not help and the command is required
  # EXTERNAL packages
  # WARNING! Spack does not consider the specs. you must check versions yourself!
  packages:
    intel:
      buildable: false
      externals:
      - prefix: /share/Apps/intel-oneapi/2021/compiler/2021.3.0/linux/bin/intel64/icc
        spec: !cat [intel@2021.3.0, !cat ['%', *gcc_back]]
    slurm:
      buildable: false
      externals:
      - spec: slurm@21.08.8-2 
        prefix: /usr/local/slurm
    hcoll:
      buildable: false
      externals:
      - spec: hcoll@4.7
        prefix: /opt/mellanox/hcoll
    # prevent issue with seeming arbitrary recompile of gcc
    # previously also included the intel oneapi location at 
    #   compiler/2023.1.0/linux/ but this was not strictly required
    gcc:
      buildable: false
      externals:
      # match this to the backing compiler from the first step
      - spec: !cat
        - gcc@12.3.0 %gcc@8.5.0 ~binutils +bootstrap ~graphite ~nvptx ~piclibs
            ~profiled~ strip build_system=autotools build_type=RelWithDebInfo 
            languages=c,c++,fortran
        - &arch arch=linux-centos8-skylake_avx512
        # receive this from the command line where you must also use "spack compiler find"
        prefix: /share/Apps/a/linux-centos8-skylake_avx512/gcc-8.5.0/gcc-12.3.0-3hs5ftxmdz4bs52vlgx54m4w3w4ankmu/
    # see issue https://github.com/spack/spack/issues/37172
    bison:
      require: "%gcc"
  # specs in a superspec format, using list of lists
  # GCC + MPI
  specs: !flatten
  - - - !cat
        - &gcc gcc@12.3.0
        - !spec {compiler: *gcc_back}
    # mpi: openmpi 4
    - - &ompi4 !cat
        - !cat
          - &ompi-base openmpi@4.1.5
          - !spec {compiler: *gcc}
          - &ompi-variants +atomics ~cuda ~cxx ~cxx_exceptions ~gpfs 
            ~internal-hwloc ~java +legacylaunchers ~lustre ~memchecker +pmi
            +romio +rsh ~singularity 
            fabrics=hcoll,ucx,ofi,cma schedulers=slurm
          - *arch
        - !spec {depends: ucx +verbs +cma +dc +dm +knem +mlx5_dv +openmp +rdmacm +thread_multiple +ud}
        # spack does not consider external slurm spec, and pmix for slurm 21 must be 3,
        #   see https://slurm.schedmd.com/mpi_guide.html#open_mpi
        - !spec {depends: pmix@3.2.3}
        - !spec {depends: &libfabric 'libfabric fabrics=tcp,verbs,udp,shm,rxd,rxm'}
        # removed python here because we are pinning against external rdma-core because rpb222
        #   checked with sma310 and he explained it is available everywhere
        #   on second thought, we are not using the external version anymore, but the hashes end
        #   up the python that supports this one is going to be used only for MPI since we are
        #   building the MPI and compilers and then treating them as external for the stack
    # mpi: openmpi 4 intranode
    - - &ompi4-intra !cat
        - *ompi-base 
        - &ompi-variants +atomics ~cuda ~cxx ~cxx_exceptions ~gpfs 
          ~internal-hwloc ~java +legacylaunchers ~lustre ~memchecker +pmi
          +romio +rsh ~singularity 
          fabrics=none schedulers=slurm
        - *arch
        # see comment above regarding pmix
        - !spec {depends: pmix@3.2.3}
  # INTEL + MPI
  - - - !cat
        - &intel-compiler intel-oneapi-compilers@2023.1.0
        - arch=linux-centos8-skylake_avx512
        - !spec {compiler: *gcc_back}
    # mpi: openmpi 4
    - - &ompi4 !cat
        - !cat
          - &mv2-base mvapich2@2.3.7-1
          - !spec {compiler: &intel oneapi@2023.1.0}
          - &mv2-variants ~alloca ~cuda ~debug +regcache +wrapperrpath ch3_rank_bits=32 
            fabrics=mrail file_systems=auto process_managers=hydra threads=single
          - *arch
  config:
    build_stage:
    - $LOCAL_SCRATCH
    - $TMPDIR
