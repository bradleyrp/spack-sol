spack:
  # CUSTOM MODULES
  # these modifications blacklist the system compiler to make Core packages
  # this streamlines the lmod tree that we expose to the users
  # dev: could have sworn you could not set this, but now it is required or else
  #   we have collisions
  view: false
  concretizer: 
    # note that things were unified without asking when we ask for two very
    #   similar openmpi packages, but we could probably switch to when_possible
    unify: false
    reuse: false
  modules:
    # configurations for a module set
    default:
      roots:
        lmod: ./lmod
      arch_folder: false
      lmod:
        core_compilers:
        - &gcc_back !system_compiler
        hierarchy:
        - compiler
        - mpi
        exclude:
        - !cat ['%', *gcc_back]
        - lmod
        include:
        - gcc
        - openmpi
        - intel-oneapi-compilers
        - intel-oneapi-mpi
        exclude_implicits: true
        projections:
          all: '{name}/{version}'
          'openmpi fabrics=none': '{name}-intra/{version}'
        hash_length: 0
  config:
    build_stage:
    - $LOCAL_SCRATCH
    - $TMPDIR
  # EXTERNAL packages
  packages:
    slurm:
      buildable: false
      externals:
      - spec: slurm@21.08.8-2 
        prefix: /usr/local/slurm
    hcoll:
      buildable: false
      externals:
      - spec: hcoll@4.8
        prefix: /opt/mellanox/hpcx/hcoll
    ucx: 
      buildable: false
      externals:
      # we defer the specs to the consumer, openmpi below, where we
      #   specify all of the features we think that the rpm provides
      # note that spack does not detect the specs, so you need to
      #   figure these out yourself before pinning against external
      # note that ucx is also in /opt/mellanox/hpcx/ucx but this
      #   causes a weird error (cannot find -liberty)
      - prefix: /usr
        spec: ucx@1.16.0
    # see issue https://github.com/spack/spack/issues/37172
    bison:
      require: "%gcc"
  # specs in a superspec format, using list of lists
  # GCC + MPI
  specs: !flatten
  - - - !cat
        - &gcc gcc@12.3.0 
        - !spec {compiler: *gcc_back}
        - &arch arch=linux-centos8-icelake
    # systemwide Lmod
    # note: we got errors on make man_pages until we unset LMOD_PACKAGE_PATH in the
    #   shell where we were running spack install because this was pointing to a 
    #   previous LMOD and it was lacking some master something package. for reference
    #   this took about an hour to solve after manually turning the crank in the
    #   spack stage directory
    - !compiled
      compiler: *gcc
      specs: 
      - !cat
        - lmod@8.7.24 auto_swap=true build_system=autotools  
        - arch=linux-centos8-haswell
    # gcc CUDA 
    - !compiled
      compiler: !cat [*gcc, *arch]
      specs:
      - &cuda cuda@12.2.1
    # mpi: openmpi 4
    - - &ompi4 !cat
        - !cat
          - &ompi-base openmpi@4.1.6
          - !spec {compiler: *gcc}
          - &ompi-variants +atomics +cuda ~cxx ~cxx_exceptions ~gpfs 
            ~internal-hwloc ~java +legacylaunchers ~lustre ~memchecker +pmi
            +romio +rsh ~singularity 
            fabrics=cma,hcoll,knem,ucx,xpmem
            schedulers=slurm
          - *arch
        # spack does not consider external slurm spec, and pmix for slurm 21 must be 3,
        #   see https://slurm.schedmd.com/mpi_guide.html#open_mpi
        - !spec {depends: pmix@3.2.3}
        - !spec {depends: *cuda}
        - !spec
          depends:
            ucx@1.16.0
            ~assertions ~backtrace_detail build_system=autotools
            +cma +cuda +dc ~debug +dm +gdrcopy ~gtest +ib_hw_tm
            ~java +knem libs=shared,static
            ~logging +mlx5_dv +openmp opt=3 +optimizations
            ~parameter_checking +pic +rc +rdmacm ~rocm simd=auto
            +thread_multiple ~ucg +ud +verbs +xpmem
            cuda_arch=89
    # include microbenchmarks with the MPI build-s03-p02.yaml stage for quick testing
    - - !cat
        - osu-micro-benchmarks
        - !spec {depends: *ompi4}
  # INTEL + MPI
  # note that I installed the compilers at one version and oneapi is detected at another
  - - - !cat
        - &intel-compiler intel-oneapi-compilers@2023
        - *arch
        - !spec {compiler: *gcc_back}
    - - &intel-mpi !cat
        - !cat
          - &intel-mpi-base intel-oneapi-mpi@2021.10.0
          - !spec 
            compiler: &intel oneapi@2023
          - &intel-mpi-variants 
            +envmods ~external-libfabric ~generic-names ~ilp64 
            build_system=generic 
          - *arch
    # Intel CUDA 
    - !compiled
      compiler: !cat [*intel, *arch]
      specs:
      - &cuda cuda@12.2.1
    # include microbenchmarks with the MPI build-s03-p02.yaml stage for quick testing
    - - !cat
        - osu-micro-benchmarks
        - !spec {depends: *intel-mpi}
