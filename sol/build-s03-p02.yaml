spack:
  # CUSTOM MODULES
  # these modifications blacklist the system compiler to make Core packages
  # this streamlines the lmod tree that we expose to the users
  # dev: could have sworn you could not set this, but now it is required or else
  #   we have collisions
  view: false
  concretizer: 
    # note that things were unified without asking when we ask for two very
    #   similar openmpi packages, but we could probably switch to when_possible
    unify: false
    reuse: false
  modules:
    # configurations for a module set
    default:
      roots:
        lmod: ./lmod
      arch_folder: false
      lmod:
        core_compilers:
        - &gcc_back !system_compiler
        hierarchy:
        - compiler
        - mpi
        exclude:
        - !cat ['%', *gcc_back]
        include:
        - gcc
        - openmpi
        - intel-oneapi-compilers
        - intel-oneapi-mpi
        exclude_implicits: true
        projections:
          all: '{name}/{version}'
          'openmpi fabrics=none': '{name}-intra/{version}'
        hash_length: 0
  config:
    build_stage:
    - $LOCAL_SCRATCH
    - $TMPDIR
  # dev: try match to external gcc via: spack compiler find --scope env:build <path>
  #   however pasting that in here does not help and the command is required
  # EXTERNAL packages
  # WARNING! Spack does not consider the specs. you must check versions yourself!
  packages:
    slurm:
      buildable: false
      externals:
      - spec: slurm@21.08.8-2 
        prefix: /usr/local/slurm
    hcoll:
      buildable: false
      externals:
      - spec: hcoll@4.8
        prefix: /opt/mellanox/hpcx/hcoll
    # prevent issue with seeming arbitrary recompile of gcc
    # previously also included the intel oneapi location at 
    #   compiler/2023.1.0/linux/ but this was not strictly required
    #! gcc:
    #!   buildable: false
    #!   externals:
    #!   # match this to the backing compiler from the first step
    #!   - spec: !cat
    #!     - gcc@13.2.0 %gcc@8.5.0 ~binutils +bootstrap ~graphite ~nvptx ~piclibs
    #!       ~profiled ~strip build_system=autotools build_type=RelWithDebInfo
    #!       languages=c,c++,fortran
    #!     - &arch arch=linux-centos8-icelake
    #!     # receive this from the command line where you must also use "spack compiler find"
    #!     prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-8.5.0/gcc-13.2.0-olmxr4tsvnilzqqv65gwuilaaij6hbcc
    # see issue https://github.com/spack/spack/issues/37172
    bison:
      require: "%gcc"
    # note that our new ice lake nodes have external ucx
    #   but I found it easy enough to compile our own
    #   to use the external, match the specs and use prefix /usr
  # specs in a superspec format, using list of lists
  # GCC + MPI
  specs: !flatten
  - - - !cat
        - &gcc gcc@12.3.0
        - !spec {compiler: *gcc_back}
        - &arch arch=linux-centos8-icelake
    # gcc CUDA 
    - !compiled
      compiler: !cat [*gcc, *arch]
      specs:
      - &cuda cuda@12.2.1
    # mpi: openmpi 4
    - - &ompi4 !cat
        - !cat
          - &ompi-base openmpi@4.1.6
          - !spec {compiler: *gcc}
          - &ompi-variants +atomics +cuda ~cxx ~cxx_exceptions ~gpfs 
            ~internal-hwloc ~java +legacylaunchers ~lustre ~memchecker +pmi
            +romio +rsh ~singularity 
            fabrics=cma,hcoll,knem,ofi,ucx,verbs,xpmem
            schedulers=slurm
          - *arch
        - !spec 
          depends: 
            &ucx ucx@1.14.1
            ~assertions ~backtrace_detail build_system=autotools
            +cma +cuda +dc ~debug +dm ~gdrcopy ~gtest +ib_hw_tm
            ~java +knem libs=shared,static
            ~logging +mlx5_dv +openmp opt=3 +optimizations
            ~parameter_checking +pic +rc +rdmacm ~rocm simd=auto
            +thread_multiple ~ucg +ud +verbs +xpmem
            cuda_arch=89
        # spack does not consider external slurm spec, and pmix for slurm 21 must be 3,
        #   see https://slurm.schedmd.com/mpi_guide.html#open_mpi
        - !spec {depends: pmix@3.2.3}
        - !spec {depends: *cuda}
    # include microbenchmarks with the MPI build-s03-p02.yaml stage for quick testing
    - - !cat
        - osu-micro-benchmarks
        - !spec {depends: *ompi4}
  # INTEL + MPI
  # note that I installed the compilers at one version and oneapi is detected at another
  - - - !cat
        - &intel-compiler intel-oneapi-compilers@2023.2.1
        - *arch
        - !spec {compiler: *gcc_back}
    - - &intel-mpi !cat
        - !cat
          - &intel-mpi-base intel-oneapi-mpi@2021.10.0
          - !spec 
            compiler: &intel oneapi@2023.2.0
          - &intel-mpi-variants 
            +envmods ~external-libfabric ~generic-names ~ilp64 
            build_system=generic 
          - *arch
    # Intel CUDA 
    - !compiled
      compiler: !cat [*intel, *arch]
      specs:
      - &cuda cuda@12.2.1
    # include microbenchmarks with the MPI build-s03-p02.yaml stage for quick testing
    - - !cat
        - osu-micro-benchmarks
        - !spec {depends: *intel-mpi}
