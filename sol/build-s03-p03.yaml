spack:
  # CUSTOM MODULES
  # these modifications blacklist the system compiler to make Core packages
  # this streamlines the lmod tree that we expose to the users
  # dev: could have sworn you could not set this, but now it is required or else
  #   we have collisions
  view: false
  concretizer: 
    # note that things were unified without asking when we ask for two very
    #   similar openmpi packages, but we could probably switch to when_possible
    unify: false
    reuse: false
  modules:
    # configurations for a module set
    default:
      roots:
        lmod: ./lmod
      arch_folder: false
      lmod:
        core_compilers:
        - &gcc_back !system_compiler
        hierarchy:
        - compiler
        - mpi
        exclude:
        - !cat ['%', *gcc_back]
        - lmod
        - cmake
        include:
        - gcc
        - openmpi
        - intel-oneapi-compilers
        - intel-oneapi-mpi
        # exclude cmake above and include this one because we design it to be reused with
        #   intel-oneapi-mkl that supports R to reduce redundancy
        - cmake ^curl +libidn2
        exclude_implicits: true
        # whenever a modulefile refers to a custom projection
        core_specs:
        - build_system=python_pip ^python
        projections:
          gromacs +cufftmp: gromacs/{version}-cufftmp
          build_system=python_pip ^python ^mpi: &py-mpi-pro 'py-{^python.version}-{compiler.name}-{compiler.version}-{^mpi.name}-{^mpi.version}/{name}/{version}'
          build_system=python_pip ^python: &py-pro 'py-{^python.version}-{compiler.name}-{compiler.version}/{name}/{version}' 
          ^r: 'r-{^r.version}/{name}/{version}'
          namd +cuda: 'namd-gpu/{version}'
          all: '{name}/{version}'
        hash_length: 0
        # other module extensions in spack/etc/templates/lmod.lua
        #   for example the defintions of LOCAL_SCRATCH and CEPHFS_SCRATCH
        all:
          environment:
            set:
              'LURC_{name}_DIR': '{prefix}'
        # gromacs +cufftmp throws "WARN: NCCL library not found" unless we link
        nvhpc:
          environment:
            prepend_path:
              LD_LIBRARY_PATH: '{prefix}/Linux_{target.family}/{version}/comm_libs/nccl/lib'
      # recover the LD_LIBRARY_PATH behavior
      #   this was prompted by sundials
      prefix_inspections:
        ./lib64: [LD_LIBRARY_PATH]  
        ./lib: [LD_LIBRARY_PATH]
        ./include: [C_INCLUDE_PATH,CPLUS_INCLUDE_PATH]
  config:
    build_stage:
    - $LOCAL_SCRATCH
    - $TMPDIR
  # EXTERNAL packages
  packages:
    slurm:
      buildable: false
      externals:
      - spec: slurm@21.08.8-2 
        prefix: /usr/local/slurm
    hcoll:
      buildable: false
      externals:
      - spec: hcoll@4.8
        prefix: /opt/mellanox/hpcx/hcoll
    # see issue https://github.com/spack/spack/issues/37172
    bison:
      require: "%gcc"
    # PINNING
    gcc:
      buildable: false
      externals:
      - spec: gcc@12.3.0%gcc@8.5.0~binutils+bootstrap~graphite~nvptx~piclibs~profiled~strip build_system=autotools build_type=RelWithDebInfo languages=c,c++,fortran arch=linux-centos8-icelake
        prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-8.5.0/gcc-12.3.0-bsunvrosg6jgrb6oov6phkp5ptzuqqly
    openmpi:
      buildable: false
      externals: 
      # dev: received internal error even though I matched this spec to the openmpi spec at the root level below
      #   error message: internal_error("node belongs to no unification set")
      #   ...!!
      - spec: openmpi@4.1.6%gcc@12.3.0+atomics+cuda~cxx~cxx_exceptions~gpfs~internal-hwloc~internal-pmix~java+legacylaunchers~lustre~memchecker~openshmem~orterunprefix+pmi+romio+rsh~singularity+static+vt+wrapper-rpath build_system=autotools cuda_arch=89 fabrics=cma,hcoll,knem,ucx,xpmem schedulers=slurm arch=linux-centos8-icelake
        prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-12.3.0/openmpi-4.1.6-as7ukvqajqdnklhutbmh46epxpdg7uel
    intel-oneapi-compilers:
      buildable: false
      externals:
      - spec: intel-oneapi-compilers@2023.2.1%gcc@8.5.0+envmods build_system=generic arch=linux-centos8-icelake
        prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-8.5.0/intel-oneapi-compilers-2023.2.1-xzbykavhswznxe3a7rnwfociigkqbbe5
    intel-oneapi-mpi:
      buildable: false
      externals:
      - spec: intel-oneapi-mpi@2021.10.0%oneapi@2023.2.0+envmods~external-libfabric~generic-names~ilp64 build_system=generic arch=linux-centos8-icelake
        prefix: /share/Apps/ice24v1/linux-centos8-icelake/oneapi-2023.2.0/intel-oneapi-mpi-2021.10.0-umeoaxviwhl3naan6wtfayripnetzoei
#    cuda:
#      buildable: false
#      externals:
#      - spec: cuda@12.2.1%gcc@12.3.0~allow-unsupported-compilers~dev build_system=generic arch=linux-centos8-icelake
#        prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-12.3.0/cuda-12.2.1-dxwakxauuyioent6cxlfd6lqwmwiyjec
#      - spec: cuda@12.2.1%oneapi@2023.2.0~allow-unsupported-compilers~dev build_system=generic arch=linux-centos8-icelake
#        prefix: /share/Apps/ice24v1/linux-centos8-icelake/oneapi-2023.2.0/cuda-12.2.1-abvxym3jb7qg22hrhho5kzyzy325nylx
    ucx:
      buildable: false
      externals:
      - spec: !cat
        - &ucx-external ucx@1.16.0~assertions~backtrace_detail+cma+cuda+dc~debug+dm+examples+gdrcopy~gtest+ib_hw_tm~java+knem~logging+mlx5_dv+openmp+optimizations~parameter_checking+pic+rc+rdmacm~rocm+thread_multiple~ucg+ud+verbs~vfs+xpmem build_system=autotools cuda_arch=89 libs=shared,static opt=3 simd=auto arch=linux-centos8-icelake
        - '%gcc@12.3.0'
        prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-12.3.0/ucx-1.16.0-bnuqwsgvjmmvp54nzwjxxogewl4kgd2c
  # specs in a superspec format, using list of lists
  # GCC + MPI
  specs: !flatten
  - - - !cat
        - &gcc gcc@12.3.0
        - !spec {compiler: *gcc_back}
  # BUILD verbatim on top of build-s03-p02.yaml
  # depends on gcc 
  - &gcc-all
    - !compiled
      compiler: &gcc gcc@12.3.0 
      arch: &arch arch=linux-centos8-icelake
      specs: !flatten
      - &compiled-specs
        - &perl perl@5.38.0 +cpanm+open+shared+threads build_system=generic
        - !cat &cmake
          - cmake@3.27.7 
          # marking the default so that this cmake propagates to e.g. tbb, mkl
          #   and compiles a single default tbb, mkl also usable with R
          - !spec {depends: &curl-libidn2 curl +libidn2}
          - !spec {depends: *perl}
        - zlib@1.3 +optimize+pic+shared build_system=makefile
        - bzip2@1.0.8 ~debug~pic+shared build_system=generic
        - &openjdk openjdk@11.0.20.1_1
        - &tbb !cat
          - intel-tbb@2021.9.0
          - ~ipo +shared +tm
          - !spec {depends: *cmake}
    # gcc CUDA and ucx for openmpi
    - !compiled
      compiler: *gcc
      arch: *arch
      specs:
      - &cuda cuda@12.2.1
      - &nvhpc nvhpc@23.9 +blas+lapack~mpi
#      - &ucx !cat
#        - ucx@1.16.0
#        - ~assertions ~backtrace_detail build_system=autotools
#          +cma +cuda +dc ~debug +dm +gdrcopy ~gtest +ib_hw_tm
#          ~java +knem libs=shared,static
#          ~logging +mlx5_dv +openmp opt=3 +optimizations
#          ~parameter_checking +pic +rc +rdmacm ~rocm simd=auto
#          +thread_multiple ~ucg +ud +verbs +xpmem
#          cuda_arch=89
#        - !spec {depends: *cuda}
#        - !spec {depends: hwloc +cuda}
    # mpi: openmpi 4
    - - &ompi4 !cat
        - !cat
          - &ompi-base openmpi@4.1.6
          - !spec {compiler: *gcc}
          - &ompi-variants +atomics +cuda ~cxx ~cxx_exceptions ~gpfs 
            ~internal-hwloc ~java +legacylaunchers ~lustre ~memchecker +pmi
            +romio +rsh ~singularity 
            fabrics=cma,hcoll,knem,ucx,xpmem
            schedulers=slurm
            cuda_arch=89
          - *arch
        # spack does not consider external slurm spec, and pmix for slurm 21 must be 3,
        #   see https://slurm.schedmd.com/mpi_guide.html#open_mpi
#        - !spec {depends: pmix@3.2.3}
#        # - !spec {depends: *ucx}
#        - !spec {depends: *cuda}
    # gcc exclusives 
    - !compiled
      compiler: *gcc
      arch: *arch
      specs:
      - !cat &openblas
        - openblas@0.3.24 
        - ~bignuma ~consistent_fpcsr +fortran ~ilp64 +locking +pic +shared
      - anaconda3@2023.09-0 
      - &eigen eigen
      - pugixml
      - !cat &metis 
        - !cat
          - metis@5.1.0
          # defensively adding the compiler and arch otherwise it applies to the curl
          #   dependency only
          # dev: we need to place the compiler earlier in the list or it applies downstream
          #   see the abandoned spec_reformed function which sought to address this until
          #   we cleaned up the tree
          - *arch
        - ~gdb ~int64 ~ipo ~real64 +shared
        - !spec {depends: *curl-libidn2}
  # INTEL + MPI
  # note that I installed the compilers at one version and oneapi is detected at another
  - - - !cat
        - &intel-compiler intel-oneapi-compilers@2023
        - *arch
        - !spec {compiler: *gcc_back}
    - - &intel-mpi !cat
        - !cat
          - &intel-mpi-base intel-oneapi-mpi@2021.10.0
          - !spec {compiler: &intel oneapi@2023}
          - &intel-mpi-variants 
            +envmods ~external-libfabric ~generic-names ~ilp64 
            build_system=generic 
          - *arch
    # Intel CUDA 
    - !compiled
      compiler: *intel
      arch: *arch
      specs:
      - &cuda cuda@12.2.1
  # depends on gcc, openmpi
  - - !loop_depends
      base: *ompi4
      specs: &mpi-general 
      - osu-micro-benchmarks@7.1-1
      - !cat &hdf5 
        - hdf5@1.14.1-2 +cxx +fortran +hl 
      - !cat &netcdf-c
        - netcdf-c@4.9.2
        - !spec {depends: *hdf5}
      - !cat
        - &netcdf-cxx netcdf-cxx4@4.3.1
        - !spec {depends: *netcdf-c}
      - &mkl !cat 
        - &mkl-base intel-oneapi-mkl@2023.2.0
        - &mkl-variants +cluster +envmods ~ilp64 +shared threads=tbb mpi_family=openmpi
        - !spec {depends: *tbb}
        #! - !spec 
        #!   - depends: !cat
        #!     - hwloc +cuda
        #!     - !spec {depends: *cuda}
      - &netcdf-fortran !cat
        - netcdf-fortran@4.6.1 ~doc +pic +shared
        - !spec {depends: *netcdf-c}
      - &parallel-netcdf parallel-netcdf@1.12.3 ~burstbuffer +cxx +fortran +pic +shared
  # gcc python
  - - !compiled
      compiler: *gcc
      arch: *arch
      specs:
      - !cat &python
        - python@3.11.6
        - build_system=generic +bz2 +ctypes +dbm ~debug +libxml2 +lzma ~nis
          +optimizations +pic +pyexpat +pythoncmd +readline +shared
          +sqlite3 +ssl +uuid +zlib
  - - !loop_depends
      base: !cat
      - *python
      - !spec 
        compiler: *gcc
        arch: *arch
      # note that the wheel, pip, setuptools, supporting cython are ad hoc here,
      #   but for intel below we make them more explicit
      specs: &python-support
      - py-setuptools@68.0.0
      - py-wheel@0.41.2
      - py-pip@23.1.2
      - &py-cython py-cython@3.0.4
      - &py-cython-29 py-cython@0.29.36
  - - !loop_depends
      base: *ompi4
      specs:
      - !cat &py-numpy
        - py-numpy@1.26.1
        - !spec {depends: *mkl}
        - !spec {depends: *py-cython-29}
        - !spec {depends: *python}
      - !cat &py-scipy
        - py-scipy@1.11.3
        - !spec {depends: *py-numpy}
  # gcc R
  - - !compiled
      compiler: *gcc
      arch: *arch
      specs:
      - !cat &r
        - r@4.3.0
        - build_system=autotools +external-lapack
        # constrain cmake to reuse tbb and mkl below
        - !spec {depends: cmake ^curl+libidn2}
        - !spec {depends: *ompi4}
        - !spec {depends: *openjdk}
        # something triggers a change to mkl
        - !spec {depends: *mkl}
        - !spec {depends: *tbb}
  # depends on gcc, openmpi, gcc-exclusive
  - - !loop_depends
      base: *ompi4
      specs: 
      - !cat &py-mpi4py 
        - py-mpi4py@3.1.4
        - !spec {depends: *py-cython-29}
        - !spec {depends: *python}
      - !cat &parmetis 
        - parmetis@4.0.3
        - !spec {depends: *metis}
  - - !compiled
      compiler: *intel
      arch: *arch
      specs: *compiled-specs 
  # depends on gcc, openmpi, mkl, cuda
  - - !builder
      # note thinking about changing target to one with AVX2_256 because GROMACS
      #   produces a warning: 
      #     Compiled SIMD is AVX_512, but AVX2_256 might be faster (see log).
      #   however we have 2 FMA units on our processors so 512 should be faster
      suffix: !spec {depends: *ompi4}
      tail: *mkl
      specs:
      - !cat
        - &gmx-base gromacs@2023.3
        - &gmx-variants build_system=cmake build_type=Release
          ~cp2k +cuda cuda_arch=89 ~cycle_subcounters
          ~double +hwloc ~intel_provided_gcc ~mdrun_only
          +mpi ~nosuffix ~opencl +openmp openmp_max_threads=none 
          ~relaxed_double_precision
          +shared ~sycl
          ~cufftmp
        - !spec {depends: *cuda}
      # alternate copy of gromacs with cuFFTMp
      - !cat
        - &gmx-base gromacs@2023.3
        - &gmx-variants build_system=cmake build_type=Release
          ~cp2k +cuda cuda_arch=89 ~cycle_subcounters
          ~double +hwloc ~intel_provided_gcc ~mdrun_only
          +mpi ~nosuffix ~opencl +openmp openmp_max_threads=none 
          ~relaxed_double_precision
          +shared ~sycl
          +cufftmp
        - !spec {depends: *nvhpc}
        - !spec {depends: *cuda}
  # depends on gcc, openmpi, mkl, cuda
  - - !builder
      suffix: !spec {depends: *ompi4}
      tail: *mkl
      specs:
      - !cat
        - &lammps-base lammps@20230802 
        - &lammps-variant
          +asphere +body  build_system=cmake build_type=RelWithDebInfo
          +class2 +colloid +compress +coreshell
          +cuda  cuda_arch=89 +cuda_mps 
          +dipole +exceptions ~ffmpeg +granular ~ipo ~jpeg 
          +kim ~kokkos 
          +kspace fftw_precision=double
          lammps_sizes=smallbig 
          +lib +manybody +mc +misc +molecule +mpi +mpiio +openmp
          +opt +peri +png +poems +python +qeq +replica +rigid ~rocm +shock +srd +voronoi
          +atc +h5md +meam +netcdf +phonon +qtb +reaxff
          +extra-pair
        - !spec {depends: *python}
        - !spec {depends: *py-mpi4py}
        - !spec {depends: *netcdf-c}
        # dev: errors on kokkos 
        #   basically cmake says it cannot get the architecture, even though the value is set properly
        #   - !spec {depends: kokkos +wrapper +cuda cuda_arch=89}
        - !spec {depends: *cuda}
        - !spec {depends: *openblas}
      # building generic cast of characters
      - &hypre hypre@2.20.0 +openmp
      - !cat &superlu-dist
        - superlu-dist@8.1.2
        - !spec {depends: *metis}
        - !spec {depends: *parmetis}
      - !cat &petsc 
        - petsc@3.20.1
        - !spec {depends: *hdf5}
        - !spec {depends: *hypre}
        - !spec {depends: *metis}
        - !spec {depends: *parmetis}
        - !spec {depends: *superlu-dist}
        - !spec {depends: *python}
      - !cat &boost
        - boost@1.83.0
          ~atomic ~chrono ~clanglibcpp ~container ~context ~contract ~coroutine ~date_time
          ~debug ~exception ~fiber ~filesystem ~graph ~graph_parallel ~icu ~iostreams 
          +json +locale ~log +math +mpi +multithreaded ~nowide +numpy ~pic
          ~program_options +python ~random +regex +serialization +shared ~signals 
          ~singlethreaded ~stacktrace ~system ~taggedlayout +test ~thread ~timer 
          ~type_erasure ~versionedlayout ~wave 
        - !spec {depends: *py-numpy}
  # requirements for aLENS for div206
  - - !loop_depends
      base: *ompi4
      specs: 
      # dev: aLENS specifically requests trilinos@12.18 but I get errors
      - !cat
        - vtk@9.2.6 
        - !spec {depends: *netcdf-c}
        # note that vtk depends on netcdf-cxx@4 and we have netcdf-cxx4 instead
        - !spec {depends: *eigen}
      # - !cat &kokkos
      #   - kokkos@4.1.00
      #     ~aggressive_vectorization ~compiler_warnings ~cuda ~debug 
      #     ~debug_bounds_check ~debug_dualview_modify_check ~deprecated_code 
      #     ~examples ~hpx ~hpx_async_dispatch +hwloc ~ipo ~memkind 
      #     ~numactl +openmp +openmptarget ~pic ~rocm +serial +shared 
      #     ~sycl ~tests ~threads ~tuning ~wrapper 
      #   # the following requirements synchronize this kokkos with trilinos below
      #   #   but we end up playing whack-a-mole. if we use this, we can expose this kokkos to
      #   #   the module system and reuse it in support of trilinos. but then it tries to
      #   #   recompile MKL. since we cannot have both, and kokkos is 7MB, we just drop this
      #   #   requirement here. note that in the future we should probably recompile EVERYTHING
      #   #   with: cmake ^curl +libidn2 and hwloc +netloc
      #   #! - !spec {depends: *curl-libidn2}
      #   #! - !spec {depends: hwloc +netloc}
      - !cat &trilinos
        - trilinos@14.4.0
          +zoltan +zoltan2
        - !spec {depends: *mkl}
      #! dev: cannot compile this because it needs gmake@4.2.1
      #! - !cat &trilinos-12
      #!   - trilinos@12.18.1
      #!     +zoltan +zoltan2
      #!   - !spec {depends: *mkl}
      #!   - !spec {depends: gmake@4.2.1}
      #!   # dev: discarded explicit kokkos in modules: - !spec {depends: *kokkos}
  #!     - !cat &boost
  #!       - boost@1.83.0
  #!         ~atomic ~chrono ~clanglibcpp ~container ~context ~contract ~coroutine ~date_time
  #!         ~debug ~exception ~fiber ~filesystem ~graph ~graph_parallel ~icu ~iostreams 
  #!         +json +locale ~log +math +mpi +multithreaded ~nowide +numpy ~pic
  #!         ~program_options +python ~random +regex +serialization +shared ~signals 
  #!         ~singlethreaded ~stacktrace ~system ~taggedlayout +test ~thread ~timer 
  #!         ~type_erasure ~versionedlayout ~wave 
  #!       - !spec {depends: *mkl}
  #!       - !spec {depends: *py-numpy}
  #!     - !cat &kokkos
  #!       - kokkos@4.1.00
  #!         ~aggressive_vectorization ~compiler_warnings ~cuda ~debug 
  #!         ~debug_bounds_check ~debug_dualview_modify_check ~deprecated_code 
  #!         ~examples ~hpx ~hpx_async_dispatch +hwloc ~ipo ~memkind 
  #!         ~numactl +openmp +openmptarget ~pic ~rocm +serial +shared 
  #!         ~sycl ~tests ~threads ~tuning ~wrapper 
  #!     - &trilinos-base !cat
  #!       - trilinos@14.4.0
  #!       - ~adios2
  #!     - !cat &adios
  #!       - adios2@2.9.2
  #!         ~aws +blosc2 +bzip2 ~cuda ~dataspaces +fortran +hdf5 ~ipo +kokkos 
  #!         +libcatalyst +libpressio +mgard +mpi ~pic +png ~python ~rocm +sst +sz +zfp
  #!       - !spec {depends: *kokkos}
  #!       - !spec {depends: *trilinos-base}
  #!     - !cat 
  #!       - trilinos@14.4.0
  #!         ~adelus +adios2 +amesos +amesos2 +anasazi +aztec ~basker +belos 
  #!         ~boost ~chaco ~complex ~cuda ~cuda_rdc ~debug ~dtk +epetra 
  #!         +epetraext ~epetraextbtf ~epetraextexperimental 
  #!         ~epetraextgraphreorderings ~exodus +explicit_template_instantiation 
  #!         ~float +fortran ~gtest +hdf5 +hypre +ifpack +ifpack2 ~intrepid 
  #!         ~intrepid2 ~ipo ~isorropia +kokkos ~mesquite ~minitensor +ml 
  #!         +mpi +muelu ~mumps ~nox ~openmp ~panzer ~phalanx ~piro 
  #!         ~python ~rocm ~rocm_rdc ~rol ~rythmos +sacado ~scorec 
  #!         ~shards +shared ~shylu ~stk ~stokhos ~stratimikos ~strumpack 
  #!         ~suite-sparse ~superlu ~superlu-dist ~teko ~tempus ~test 
  #!         ~thyra +tpetra ~trilinoscouplings ~wrapper ~x11 ~zoltan 
  #!         ~zoltan2
  #!       # avoiding adios because this also depends on trilinos
  #!       - !spec {depends: *kokkos}
  #!       - !spec {depends: *boost}
  #!       - !spec {depends: !cat [*hdf5, /g7duo3s]}
  #!       - !spec {depends: !cat [*hypre, /kvsk5hr]}
  #!       - !spec {depends: *mkl}
  #!       - !spec {depends: !cat [*metis, /2pvshtd]}
  #!       - !spec {depends: *adios]}
  # namd on gcc
  #! - - !compiled
  #!     compiler: *gcc
  #!     specs:
  #!     - !cat
  #!       # note that this has been a major struggle
  #!       - namd@2.14 
  #!       - !cat &namd-support 
  #!         - ~single_node_gpu +cuda cuda_arch=89 interface=tcl
  #!         - !spec {depends: *mkl}
  #!         - !spec {depends: *ompi4}
  #!         - !spec {depends: *cuda}
  #!         # it is extremely counterintuitive but namd +cuda does not use charmpp +cuda
  #!         #   according to some paper I found
  #!         # iteratively built this recipe with charmpp last
  #!         # dev: we do not have pmix.h and backend=ucx pmi=slurmpmi2 says you cannot use
  #!         #   ompi and you need an impi instead. perhaps mvapich2? in the meantime, doing smp
  #!         - !spec 
  #!           depends: !cat
  #!           # ucx requires pmi but pmi=slurmpmi2 requires a non-openmpi MPI
  #!           #   so I give up on ucx and try verbs
  #!           # ucx is probably best, see
  #!           #   https://www.ks.uiuc.edu/Research/namd/wiki/index.cgi?NamdOnInfiniBand
  #!           - charmpp +smp backend=verbs
  #!           # - !spec {depends: *ucx-external}
  # intel python
  - - !compiled
      compiler: *intel
      arch: *arch
      specs:
      # due to a compiler bug in oneapi explained in the Python Spack package,
      #   we have to hang out in 2019
      - &python-intel python@3.8.18
  - - !loop_depends
      base: !cat
      - *python-intel
      - !spec 
        compiler: *intel
        arch: *arch
      specs: &python-support-intel
      # suport for py-cython and hence py-numpy and scipy below is standardized
      #   around a single py-pip, py-wheel, and py-setuptools at maximum version
      #   for Python 3.8 which is the highest we can go with %oneapi
      - &py-setuptools-intel py-setuptools@63.4.3
      - &py-wheel-intel py-wheel@0.37.1
      - &py-pip-intel py-pip@23.1.2
      - !cat &py-cython-29-intel
        - py-cython@0.29.36
        - !spec {depends: *py-setuptools-intel}
        - !spec {depends: *py-wheel-intel}
        - !spec {depends: *py-pip-intel}
        - !spec {depends: *python-intel}
  - - !compiled
      compiler: *intel
      arch: *arch
      specs:
      - !cat &py-numpy-intel
        - py-numpy@1.24.4
        - !spec {depends: *mkl}
        - !spec {depends: *py-cython-29-intel}
        - !spec {depends: *intel-mpi-base}
      - !cat &py-scipy-intel
        - py-scipy@1.10.1
        - !spec {depends: *py-cython-29-intel}
  # depends on intel, intel-mpi
  - - !loop_depends
      base: *intel-mpi
      specs: *mpi-general 
  # depends on intel, intel-mpi, exclusives
  #! icx is not supported in this version. also I get errors with MPI and the docs indicate that
  #!   you need some kind of icc architecture for openmpi anyway
  #! - - !loop_depends
  #!     base: *intel-mpi
  #!     specs: 
  #!     - !cat
  #!       - namd@2.14 
  #!       - !cat &namd-support 
  #!         - +single_node_gpu +cuda cuda_arch=89 interface=tcl
  #!         - !spec &charmpp-dep
  #!           depends: !cat
  #!           - charmpp
  #!           - +cuda +omp ~papi 
  #!             backend=multicore
  #!             +production ~pthreads 
  #!             +shared +smp ~syncft ~tcp ~tracing 
  #!             build-target=LIBS build_system=generic
  #!           - !spec {depends: *cuda}
  #!         - !spec {depends: *cuda}
  #!         - !spec {depends: *mkl}
  #!     - !cat
  #!       - namd@3.0b3 
  #!       - *namd-support
  #!       - *charmpp-dep
  # REQUEST: sundials for ada823
  # building components for petsc
  #! - - !compiled
  #!     compiler: *intel
  #!     specs:
  #!     - *openblas
  #!     - &metis !cat
  #!       - metis@5.1.0 
  # depends on intel, intel-mpi, exclusives
  - - !loop_depends
      base: *intel-mpi
      specs: 
  #!     - *hdf5 
  #!     - *parallel-netcdf
  #!     - *netcdf-fortran
  #!     - &parmetis !cat
  #!       - parmetis@4.0.3
  #!     - &superlu !cat
  #!       - superlu@5.3.0
  #!       - !spec {depends: *mkl}
  #!     - !cat &petsc
  #!       - petsc@3.20.1
  #!       - ~X ~batch ~cgns ~complex ~cuda ~debug +double ~exodusii
  #!         +fftw +fortran ~giflib +hdf5 ~hpddm ~hwloc +hypre +int64 ~jpeg
  #!         ~knl ~kokkos ~libpng ~libyaml ~memkind +metis +mkl-pardiso ~mmg
  #!         ~moab ~mpfr +mpi ~mumps ~openmp ~p4est ~parmmg ~ptscotch ~random123 ~rocm
  #!         ~saws ~scalapack +shared ~strumpack ~suite-sparse +superlu-dist ~sycl
  #!         ~tetgen ~trilinos ~valgrind 
  #!         build_system=generic clanguage=C memalign=none 
  #!       # - !spec {depends: *hdf5}
  #!       - !spec {depends: *metis}
  #!       - !spec {depends: *parmetis}
  #!       - !spec {depends: *python-intel}
  #!       - !spec {depends: *superlu}
      - &hypre-intel !cat
        - hypre@2.20.0 +openmp +int64
        - !spec {depends: *mkl}
  #! giving up after major struggle
  #! - - !compiled
  #!     compiler: *intel
  #!     specs:
  #!     - !cat
  #!       - sundials@2.7.0
  #!       - +ARKODE +CVODE +CVODES +IDA +IDAS +KINSOL
  #!         ~mpi +static +shared
  #!         +fcmix
  #!       #! - !spec {depends: *hypre}
  #!       #! - !spec {depends: *petsc}
  #! # depends on intel, intel-mpi, exclusives
  #! - - !loop_depends
  #!     base: *intel-mpi
  #!     specs: 
  #!     # dev: manually downloaded the source and updated Spack
  #!     #   via vim $(spack location -p amber)/package.py
  #!     - !cat
  #!       - amber@22 +cuda cuda_arch=89 +mpi ~openmp +update
  #!       # interestingly, you need to specify intel here or most of
  #!       #   this tree supporting Amber is %gcc
  #!       - !spec {compiler: *intel}
  #!       - !spec {depends: *cuda}
  #!       - !spec {depends: *hdf5}
  #!       - !spec {depends: *netcdf-fortran}
  #!       - !spec {depends: *parallel-netcdf}
