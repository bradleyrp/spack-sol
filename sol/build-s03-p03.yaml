spack:
  # CUSTOM MODULES
  # these modifications blacklist the system compiler to make Core packages
  # this streamlines the lmod tree that we expose to the users
  # dev: could have sworn you could not set this, but now it is required or else
  #   we have collisions
  view: false
  concretizer: 
    # note that things were unified without asking when we ask for two very
    #   similar openmpi packages, but we could probably switch to when_possible
    unify: false
    reuse: false
  modules:
    # configurations for a module set
    default:
      roots:
        lmod: ./lmod
      arch_folder: false
      lmod:
        core_compilers:
        - &gcc_back !system_compiler
        hierarchy:
        - compiler
        - mpi
        exclude:
        - !cat ['%', *gcc_back]
        - lmod
        - cmake
        include:
        - gcc
        - openmpi
        - intel-oneapi-compilers
        - intel-oneapi-mpi
        # exclude cmake above and include this one because we design it to be reused with
        #   intel-oneapi-mkl that supports R to reduce redundancy
        - cmake ^curl +libidn2
        exclude_implicits: true
        core_specs:
        # the following are required for "osp modules"
        - build_system=generic ^python
        - py-numpy
        - py-setuptools
        - py-pip
        projections:
          py-numpy: &py-mpi-pro 'py-{^python.version}-{compiler.name}-{compiler.version}-{^mpi.name}-{^mpi.version}/{name}/{version}'
          build_system=generic ^python: &py-pro 'py-{^python.version}-{compiler.name}-{compiler.version}/{name}/{version}' 
          py-pip: *py-pro
          py-setuptools: *py-pro
          ^r: 'r-{^r.version}/{name}/{version}'
          all: '{name}/{version}'
        hash_length: 0
  config:
    build_stage:
    - $LOCAL_SCRATCH
    - $TMPDIR
  # EXTERNAL packages
  packages:
    slurm:
      buildable: false
      externals:
      - spec: slurm@21.08.8-2 
        prefix: /usr/local/slurm
    hcoll:
      buildable: false
      externals:
      - spec: hcoll@4.8
        prefix: /opt/mellanox/hpcx/hcoll
    ucx: 
      buildable: false
      externals:
      # we defer the specs to the consumer, openmpi below, where we
      #   specify all of the features we think that the rpm provides
      # note that spack does not detect the specs, so you need to
      #   figure these out yourself before pinning against external
      # note that ucx is also in /opt/mellanox/hpcx/ucx but this
      #   causes a weird error (cannot find -liberty)
      - prefix: /usr
        spec: ucx@1.16.0
    # see issue https://github.com/spack/spack/issues/37172
    bison:
      require: "%gcc"
    #! # dev: pinning prebuilt codes to avoid recompile. sporadically necessary
    #! gcc:
    #!   buildable: false
    #!   externals:
    #!   - prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-8.5.0/gcc-12.3.0-bsunvrosg6jgrb6oov6phkp5ptzuqqly 
    #!     spec: gcc@12.3.0%gcc@8.5.0~binutils+bootstrap~graphite~nvptx~piclibs~profiled~strip build_system=autotools build_type=RelWithDebInfo languages=c,c++,fortran arch=linux-centos8-icelake
    intel-oneapi-compilers:
      buildable: false
      externals:
      - prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-8.5.0/intel-oneapi-compilers-2023.2.1-xzbykavhswznxe3a7rnwfociigkqbbe5
        spec: intel-oneapi-compilers@2023.2.1%gcc@8.5.0+envmods build_system=generic arch=linux-centos8-icelake
    #! # dev: pinning prebuilt codes to avoid recompile. sporadically necessary
    #! openmpi:
    #!   buildable: false
    #!   externals:
    #!   - prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-12.3.0/openmpi-4.1.6-sesoyvfctcfshpreipyaaiftjrf6dwxq 
    #!     spec: !cat
    #!     - &ompi openmpi@4.1.6+atomics+cuda~cxx~cxx_exceptions~gpfs~internal-hwloc~internal-pmix~java+legacylaunchers~lustre~memchecker~openshmem~orterunprefix+pmi+romio+rsh~singularity+static+vt+wrapper-rpath build_system=autotools cuda_arch=none fabrics=cma,hcoll,knem,ucx,xpmem schedulers=slurm arch=linux-centos8-icelake
    #!     - !spec {'compiler': gcc@12.3.0}
    #! # dev: pinning prebuilt codes to avoid recompile. sporadically necessary
    #! intel-oneapi-mpi:
    #!   buildable: false
    #!   externals:
    #!   - prefix: /share/Apps/ice24v1/linux-centos8-icelake/oneapi-2023.2.0/intel-oneapi-mpi-2021.10.0-umeoaxviwhl3naan6wtfayripnetzoei 
    #!     spec: intel-oneapi-mpi@2021.10.0%oneapi@2023.2.0+envmods~external-libfabric~generic-names~ilp64 build_system=generic arch=linux-centos8-icelake
    #! # dev: pinning prebuilt codes to avoid recompile. sporadically necessary
    #! cuda:
    #!   buildable: false
    #!   externals:
    #!   - prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-12.3.0/cuda-12.2.1-dxwakxauuyioent6cxlfd6lqwmwiyjec
    #!     spec: cuda@12.2.1%gcc@12.3.0~allow-unsupported-compilers~dev build_system=generic arch=linux-centos8-icelake
    #!   - prefix: /share/Apps/ice24v1/linux-centos8-icelake/oneapi-2023.2.0/cuda-12.2.1-abvxym3jb7qg22hrhho5kzyzy325nylx
    #!     spec: cuda@12.2.1%oneapi@2023.2.0~allow-unsupported-compilers~dev build_system=generic arch=linux-centos8-icelake 
  # specs in a superspec format, using list of lists
  # GCC + MPI
  specs: !flatten
  - - - !cat
        - &gcc gcc@12.3.0
        - !spec {compiler: *gcc_back}
  # BUILD verbatim on top of build-s03-p02.yaml
  # depends on gcc 
  - &gcc-all
    - !compiled
      compiler: !cat
      - &gcc gcc@12.3.0 
      - &arch arch=linux-centos8-icelake
      specs: !flatten
      - &compiled-specs
        - &perl perl@5.38.0 +cpanm+open+shared+threads build_system=generic
        - !cat &cmake
          - cmake@3.27.7 
          # marking the default so that this cmake propagates to e.g. tbb, mkl
          #   and compiles a single default tbb, mkl also usable with R
          - !spec {depends: curl +libidn2}
          - !spec {depends: *perl}
        - zlib@1.3 +optimize+pic+shared build_system=makefile
        - bzip2@1.0.8 ~debug~pic+shared build_system=generic
        - &openjdk openjdk@11.0.20.1_1
    # gcc exclusives, CUDA 
    - !compiled
      compiler: !cat [*gcc, *arch]
      specs:
      - &cuda cuda@12.2.1
      - !cat &openblas
        - openblas@0.3.24 
        - ~bignuma ~consistent_fpcsr +fortran ~ilp64 +locking +pic +shared
      - &tbb !cat
        - intel-tbb@2021.9.0
        - ~ipo +shared +tm
        - !spec {depends: *cmake}
      - anaconda3@2023.09-0 
    # mpi: openmpi 4
    - - &ompi4 !cat
        - !cat
          - &ompi-base openmpi@4.1.6
          - !spec {compiler: *gcc}
          - &ompi-variants +atomics +cuda ~cxx ~cxx_exceptions ~gpfs 
            ~internal-hwloc ~java +legacylaunchers ~lustre ~memchecker +pmi
            +romio +rsh ~singularity 
            fabrics=cma,hcoll,knem,ucx,xpmem
            schedulers=slurm
          - *arch
        # spack does not consider external slurm spec, and pmix for slurm 21 must be 3,
        #   see https://slurm.schedmd.com/mpi_guide.html#open_mpi
        - !spec {depends: pmix@3.2.3}
        - !spec {depends: *cuda}
        - !spec
          depends:
            ucx@1.16.0
            ~assertions ~backtrace_detail build_system=autotools
            +cma +cuda +dc ~debug +dm +gdrcopy ~gtest +ib_hw_tm
            ~java +knem libs=shared,static
            ~logging +mlx5_dv +openmp opt=3 +optimizations
            ~parameter_checking +pic +rc +rdmacm ~rocm simd=auto
            +thread_multiple ~ucg +ud +verbs +xpmem
            cuda_arch=89
  # INTEL + MPI
  # note that I installed the compilers at one version and oneapi is detected at another
  - - - !cat
        - &intel-compiler intel-oneapi-compilers@2023
        - *arch
        - !spec {compiler: *gcc_back}
    - - &intel-mpi !cat
        - !cat
          - &intel-mpi-base intel-oneapi-mpi@2021.10.0
          - !spec {compiler: &intel oneapi@2023}
          - &intel-mpi-variants 
            +envmods ~external-libfabric ~generic-names ~ilp64 
            build_system=generic 
          - *arch
    # Intel CUDA 
    - !compiled
      compiler: !cat [*intel, *arch]
      specs:
      - &cuda cuda@12.2.1
  # depends on gcc, openmpi
  - - !loop_depends
      base: *ompi4
      specs: &mpi-general 
      - osu-micro-benchmarks@7.1-1
      - !cat &hdf5 
        - hdf5@1.14.1-2 +cxx +fortran +hl 
      - !cat &netcdf-c
        - netcdf-c@4.9.2
        - !spec {depends: *hdf5}
      - !cat
        - netcdf-cxx4@4.3.1
        - !spec {depends: *netcdf-c}
      - &mkl !cat 
        - &mkl-base intel-oneapi-mkl@2023.2.0
        - &mkl-variants +cluster +envmods ~ilp64 +shared threads=tbb mpi_family=openmpi
        - !spec {depends: *tbb}
      - &netcdf-fortran !cat
        - netcdf-fortran@4.6.1 ~doc +pic +shared
        - !spec {depends: *netcdf-c}
      - &parallel-netcdf parallel-netcdf@1.12.3 ~burstbuffer +cxx +fortran +pic +shared
  # gcc python
  - - !compiled
      compiler: !cat [*gcc, *arch]
      specs:
      - !cat &python
        - python@3.11.6
        - build_system=generic +bz2 +ctypes +dbm ~debug +libxml2 +lzma ~nis
          +optimizations +pic +pyexpat +pythoncmd +readline +shared
          +sqlite3 +ssl +uuid +zlib
  - - !loop_depends
      base: *python
      # note that the wheel, pip, setuptools, supporting cython are ad hoc here,
      #   but for intel below we make them more explicit
      specs: &python-support
      - py-setuptools@68.0.0
      - py-wheel@0.41.2
      - py-pip@23.1.2
      - &py-cython py-cython@3.0.4
      - &py-cython-29 py-cython@0.29.36
  - - !compiled
      compiler: !cat [*gcc, *arch]
      specs:
      - !cat &py-numpy
        - py-numpy@1.26.1
        - !spec {depends: *mkl}
        - !spec {depends: *py-cython-29}
        - !spec {depends: *python}
        - !spec {depends: *ompi4}
      - !cat &py-scipy
        - py-scipy@1.11.3
        - !spec {depends: *py-numpy}
  # gcc R
  - - !compiled
      compiler: !cat [*gcc, *arch]
      specs:
      - !cat &r
        - r@4.3.0
        - build_system=autotools +external-lapack
        # constrain cmake to reuse tbb and mkl below
        - !spec {depends: cmake ^curl+libidn2}
        - !spec {depends: *ompi4}
        - !spec {depends: *openjdk}
        # something triggers a change to mkl
        - !spec {depends: *mkl}
        - !spec {depends: *tbb}
  # depends on gcc, openmpi, gcc-exclusive
  - - !loop_depends
      base: *ompi4
      specs: 
      - !cat &py-mpi4py 
        - py-mpi4py@3.1.4
        - !spec {depends: *py-cython-29}
        - !spec {depends: *python}
  # depends on intel
  - - !compiled
      compiler: *intel
      specs: *compiled-specs 
  # depends on intel, intel-mpi
  - - !loop_depends
      base: *intel-mpi
      specs: *mpi-general 
  # depends on intel, intel-mpi, exclusives
  #! - - !loop_depends
  #!     base: *intel-mpi
  #!     specs: 
  #!     # dev: manually downloaded the source and updated Spack
  #!     #   via vim $(spack location -p amber)/package.py
  #!     - !cat
  #!       - amber@22 +cuda cuda_arch=89 +mpi ~openmp +update
  #!       # interestingly, you need to specify intel here or most of
  #!       #   this tree supporting Amber is %gcc
  #!       - !spec {compiler: *intel}
  #!       - !spec {depends: *cuda}
  #!       - !spec {depends: *hdf5}
  #!       - !spec {depends: *netcdf-fortran}
  #!       - !spec {depends: *parallel-netcdf}
  # depends on gcc, openmpi, mkl, cuda
  - - !builder
      suffix: !spec {depends: *ompi4}
      specs:
      - !cat
        - &gmx-base gromacs@2023.3
        - &gmx-variants build_system=cmake build_type=Release
          ~cp2k +cuda cuda_arch=89 ~cycle_subcounters
          ~double +hwloc ~intel_provided_gcc ~mdrun_only
          +mpi ~nosuffix ~opencl +openmp openmp_max_threads=none 
          ~relaxed_double_precision
          +shared ~sycl
        - !spec {depends: *cuda}
      - !cat
        - &lammps-base lammps@20230802 
        - &lammps-variant
          +asphere +body  build_system=cmake build_type=RelWithDebInfo
          +class2 +colloid +compress +coreshell
          +cuda  cuda_arch=89 +cuda_mps 
          +dipole +exceptions ~ffmpeg +granular ~ipo ~jpeg 
          +kim ~kokkos 
          +kspace fftw_precision=double
          lammps_sizes=smallbig 
          +lib +manybody +mc +misc +molecule +mpi +mpiio +openmp
          +opt +peri +png +poems +python +qeq +replica +rigid ~rocm +shock +srd +voronoi
          +atc +h5md +meam +netcdf +phonon +qtb +reaxff
        - !spec {depends: *python}
        - !spec {depends: *py-mpi4py}
        - !spec {depends: *netcdf-c}
        # dev: errors on kokkos 
        #   basically cmake says it cannot get the architecture, even though the value is set properly
        #   - !spec {depends: kokkos +wrapper +cuda cuda_arch=89}
        - !spec {depends: *cuda}
        - !spec {depends: *openblas}
      tail: *mkl
  # intel python
  - - !compiled
      compiler: !cat [*intel, *arch]
      specs:
      # due to a compiler bug in oneapi explained in the Python Spack package,
      #   we have to hang out in 2019
      - &python-intel python@3.8.18
  - - !loop_depends
      base: !cat
      - *python-intel
      - !spec {compiler: *intel}
      specs: &python-support-intel
      # suport for py-cython and hence py-numpy and scipy below is standardized
      #   around a single py-pip, py-wheel, and py-setuptools at maximum version
      #   for Python 3.8 which is the highest we can go with %oneapi
      - &py-setuptools-intel py-setuptools@63.4.3
      - &py-wheel-intel py-wheel@0.37.1
      - &py-pip-intel py-pip@23.1.2
      - !cat &py-cython-29-intel
        - py-cython@0.29.36
        - !spec {depends: *py-setuptools-intel}
        - !spec {depends: *py-wheel-intel}
        - !spec {depends: *py-pip-intel}
        - !spec {depends: *python-intel}
  - - !compiled
      compiler: !cat [*intel, *arch]
      specs:
      - !cat &py-numpy-intel
        - py-numpy@1.24.4
        - !spec {depends: *mkl}
        - !spec {depends: *py-cython-29-intel}
        - !spec {depends: *intel-mpi-base}
      - !cat &py-scipy-intel
        - py-scipy@1.10.1
        - !spec {depends: *py-cython-29-intel}
