spack:
  # CUSTOM MODULES
  # these modifications blacklist the system compiler to make Core packages
  # this streamlines the lmod tree that we expose to the users
  # dev: could have sworn you could not set this, but now it is required or else
  #   we have collisions
  view: false
  concretizer: 
    # note that things were unified without asking when we ask for two very
    #   similar openmpi packages, but we could probably switch to when_possible
    unify: false
    reuse: false
  modules:
    # configurations for a module set
    default:
      roots:
        lmod: ./lmod
      arch_folder: false
      lmod:
        core_compilers:
        - &gcc_back !system_compiler
        hierarchy:
        - compiler
        - mpi
        exclude:
        - !cat ['%', *gcc_back]
        - lmod
        - cmake
        include:
        - gcc
        - openmpi
        - intel-oneapi-compilers
        - intel-oneapi-mpi
        # exclude cmake above and include this one because we design it to be reused with
        #   intel-oneapi-mkl that supports R to reduce redundancy
        - cmake ^curl +libidn2
        # whenever a modulefile refers to a custom projection
        #   note that we use Core specs to stage the modules before modulefix functions
        core_specs:
        - build_system=python_pip ^python
        - ^r
        # it is critical that we avoid excluding implicits or for example py-matplotlib will
        #   not include the PYTHONPATH for a run dependency, py-pillow in the modulefile. for
        #   some reason this was not an issue with R and its dependent packages. note also that
        #   it is extremely tricky to test this. you need a completely new session or else a success
        #   will keep your PYTHONPATH correct between tests! make sure to test in a completely
        #   fresh session. lastly, we had to trim the modulerc files a bit to account for our
        #   module shuffling, or else we got errors when it tried to hide a python with a hash on it
        #   see sol_custom.custom.fix_python_modulerc
        exclude_implicits: false
        # this is critical to ensuring that PYTHONPATH for packages such as py-matplotlib
        #   includes all of the correct modules. this might be a change in spack v0.21 that did
        #   not make its way into the docs, see https://github.com/spack/spack/issues/42535
        hide_implicits: true
        hash_length: 0
        projections:
          gromacs +cufftmp: gromacs/{version}-cufftmp
          build_system=python_pip ^python ^mpi: &py-mpi-pro 'py-{^python.version}-{compiler.name}-{compiler.version}-{^mpi.name}-{^mpi.version}/{name}/{version}'
          build_system=python_pip ^python: &py-pro 'py-{^python.version}-{compiler.name}-{compiler.version}/{name}/{version}' 
          ^r ^mpi: 'r-{^r.version}-{compiler.name}-{compiler.version}-{^mpi.name}-{^mpi.version}/{name}/{version}'
          ^r: 'r-{^r.version}-{compiler.name}-{compiler.version}/{name}/{version}'
          namd +cuda: 'namd-gpu/{version}'
          all: '{name}/{version}'
        # gromacs +cufftmp throws "WARN: NCCL library not found" unless we link
        nvhpc:
          environment:
            prepend_path:
              LD_LIBRARY_PATH: '{prefix}/Linux_{target.family}/{version}/comm_libs/nccl/lib'
        # other module extensions are in spack/etc/templates/lmod.lua
        #   for example the defintions of LOCAL_SCRATCH and CEPHFS_SCRATCH
        all:
          autoload: all
          environment:
            set:
              'LURC_{name}_DIR': '{prefix}'
      # recover the LD_LIBRARY_PATH behavior
      #   this was prompted by sundials
      prefix_inspections:
        ./lib64: [LD_LIBRARY_PATH]  
        ./lib: [LD_LIBRARY_PATH]
        ./include: [C_INCLUDE_PATH,CPLUS_INCLUDE_PATH]
  config:
    build_stage:
    - $LOCAL_SCRATCH
    - $TMPDIR
  # EXTERNAL packages
  packages:
    slurm:
      buildable: false
      externals:
      - spec: slurm@21.08.8-2 
        prefix: /usr/local/slurm
    hcoll:
      buildable: false
      externals:
      - spec: hcoll@4.8
        prefix: /opt/mellanox/hpcx/hcoll
    # see issue https://github.com/spack/spack/issues/37172
    bison:
      require: "%gcc"
    # PINNING
    gcc:
      buildable: false
      externals:
      - spec: gcc@12.3.0%gcc@8.5.0~binutils+bootstrap~graphite~nvptx~piclibs~profiled~strip build_system=autotools build_type=RelWithDebInfo languages=c,c++,fortran arch=linux-centos8-icelake
        prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-8.5.0/gcc-12.3.0-bsunvrosg6jgrb6oov6phkp5ptzuqqly
    openmpi:
      buildable: false
      externals: 
      - spec: openmpi@4.1.6%gcc@12.3.0+atomics+cuda~cxx~cxx_exceptions~gpfs~internal-hwloc~internal-pmix~java+legacylaunchers~lustre~memchecker~openshmem~orterunprefix+pmi+romio+rsh~singularity+static+vt+wrapper-rpath build_system=autotools cuda_arch=89 fabrics=cma,hcoll,knem,ucx,xpmem schedulers=slurm arch=linux-centos8-icelake
        prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-12.3.0/openmpi-4.1.6-as7ukvqajqdnklhutbmh46epxpdg7uel
    intel-oneapi-compilers:
      buildable: false
      externals:
      - spec: intel-oneapi-compilers@2023.2.1%gcc@8.5.0+envmods build_system=generic arch=linux-centos8-icelake
        prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-8.5.0/intel-oneapi-compilers-2023.2.1-xzbykavhswznxe3a7rnwfociigkqbbe5
    intel-oneapi-mpi:
      buildable: false
      externals:
      - spec: intel-oneapi-mpi@2021.10.0%oneapi@2023.2.0+envmods~external-libfabric~generic-names~ilp64 build_system=generic arch=linux-centos8-icelake
        prefix: /share/Apps/ice24v1/linux-centos8-icelake/oneapi-2023.2.0/intel-oneapi-mpi-2021.10.0-umeoaxviwhl3naan6wtfayripnetzoei
    ucx:
      buildable: false
      externals:
      - spec: !cat
        - &ucx-external ucx@1.16.0~assertions~backtrace_detail+cma+cuda+dc~debug+dm+examples+gdrcopy~gtest+ib_hw_tm~java+knem~logging+mlx5_dv+openmp+optimizations~parameter_checking+pic+rc+rdmacm~rocm+thread_multiple~ucg+ud+verbs~vfs+xpmem build_system=autotools cuda_arch=89 libs=shared,static opt=3 simd=auto arch=linux-centos8-icelake
        - '%gcc@12.3.0'
        prefix: /share/Apps/ice24v1/linux-centos8-icelake/gcc-12.3.0/ucx-1.16.0-bnuqwsgvjmmvp54nzwjxxogewl4kgd2c
  # specs in a superspec format, using list of lists
  # GCC + MPI
  specs: !flatten
  - - - !cat
        - &gcc gcc@12.3.0
        - !spec {compiler: *gcc_back}
  # BUILD verbatim on top of build-s03-p02.yaml
  # depends on gcc 
  - &gcc-all
    - !compiled
      compiler: &gcc gcc@12.3.0 
      arch: &arch arch=linux-centos8-icelake
      specs: !flatten
      - &compiled-specs
        - &perl perl@5.38.0 +cpanm+open+shared+threads build_system=generic
        - !cat &cmake
          - cmake@3.27.7 
          # marking the default so that this cmake propagates to e.g. tbb, mkl
          #   and compiles a single default tbb, mkl also usable with R
          - !spec {depends: &curl-libidn2 curl +libidn2}
          - !spec {depends: *perl}
        - zlib@1.3 +optimize+pic+shared build_system=makefile
        - bzip2@1.0.8 ~debug~pic+shared build_system=generic
        - &openjdk openjdk@11.0.20.1_1
        - &tbb !cat
          - intel-tbb@2021.9.0
          - ~ipo +shared +tm
          - !spec {depends: *cmake}
        - &git git@2.42.0
    # gcc CUDA and ucx for openmpi
    - !compiled
      compiler: *gcc
      arch: *arch
      specs:
      - &cuda cuda@12.2.1
      - &nvhpc nvhpc@23.9 +blas+lapack~mpi
    # mpi: openmpi 4
    - - &ompi4 !cat
        - !cat
          - &ompi-base openmpi@4.1.6
          - !spec {compiler: *gcc}
          - &ompi-variants +atomics +cuda ~cxx ~cxx_exceptions ~gpfs 
            ~internal-hwloc ~java +legacylaunchers ~lustre ~memchecker +pmi
            +romio +rsh ~singularity 
            fabrics=cma,hcoll,knem,ucx,xpmem
            schedulers=slurm
            cuda_arch=89
          - *arch
    # gcc exclusives 
    - !compiled
      compiler: *gcc
      arch: *arch
      specs:
      - !cat &openblas
        - openblas@0.3.24 
        - ~bignuma ~consistent_fpcsr +fortran ~ilp64 +locking +pic +shared
      - anaconda3@2023.09-0 
      - &eigen eigen
      - pugixml
      - !cat &metis 
        - !cat
          - metis@5.1.0
          # defensively adding the compiler and arch otherwise it applies to the curl
          #   dependency only
          # dev: we need to place the compiler earlier in the list or it applies downstream
          #   see the abandoned spec_reformed function which sought to address this until
          #   we cleaned up the tree
          - *arch
        - ~gdb ~int64 ~ipo ~real64 +shared
        - !spec {depends: *curl-libidn2}
  # INTEL + MPI
  # note that I installed the compilers at one version and oneapi is detected at another
  - - - !cat
        - &intel-compiler intel-oneapi-compilers@2023
        - *arch
        - !spec {compiler: *gcc_back}
    - - &intel-mpi !cat
        - !cat
          - &intel-mpi-base intel-oneapi-mpi@2021.10.0
          - !spec {compiler: &intel oneapi@2023}
          - &intel-mpi-variants 
            +envmods ~external-libfabric ~generic-names ~ilp64 
            build_system=generic 
          - *arch
    # Intel CUDA 
    - !compiled
      compiler: *intel
      arch: *arch
      specs:
      - &cuda cuda@12.2.1
  # depends on gcc, openmpi
  - - !loop_depends
      base: *ompi4
      specs: &mpi-general 
      - osu-micro-benchmarks@7.1-1
      - !cat &hdf5 
        - hdf5@1.14.1-2 +cxx +fortran +hl 
      - !cat &netcdf-c
        - netcdf-c@4.9.2
        - !spec {depends: *hdf5}
      - !cat
        - &netcdf-cxx netcdf-cxx4@4.3.1
        - !spec {depends: *netcdf-c}
      - &mkl !cat 
        - &mkl-base intel-oneapi-mkl@2023.2.0
        - &mkl-variants +cluster +envmods ~ilp64 +shared threads=tbb mpi_family=openmpi
        - !spec {depends: *tbb}
      - &netcdf-fortran !cat
        - netcdf-fortran@4.6.1 ~doc +pic +shared
        - !spec {depends: *netcdf-c}
      - &parallel-netcdf parallel-netcdf@1.12.3 ~burstbuffer +cxx +fortran +pic +shared
  # gcc python
  - - !compiled
      compiler: *gcc
      arch: *arch
      specs:
      - !cat &python
        - python@3.11.6
        - build_system=generic +bz2 +ctypes +dbm ~debug +libxml2 +lzma ~nis
          +optimizations +pic +pyexpat +pythoncmd +readline +shared
          +sqlite3 +ssl +uuid +zlib
  - - !loop_depends
      base: !cat
      - *python
      - !spec 
        compiler: *gcc
        arch: *arch
      # note that the wheel, pip, setuptools, supporting cython are ad hoc here,
      #   but for intel below we make them more explicit
      specs: &python-support
      - py-setuptools@68.0.0
      - py-wheel@0.41.2
      - py-pip@23.1.2
      - py-packaging@23.1
      - &py-scm py-setuptools-scm@7.1.0
      - &py-cython py-cython@3.0.4
      - &py-cython-29 py-cython@0.29.36
  - - !loop_depends
      base: *ompi4
      specs:
      - !cat &py-numpy
        - py-numpy@1.26.1
        - !spec {depends: *mkl}
        - !spec {depends: *py-cython-29}
        - !spec {depends: *python}
      - !cat &py-scipy
        - py-scipy@1.11.3
        - !spec {depends: *py-numpy}
  - - !loop_depends
      base: !cat
      - *python
      - !spec 
        compiler: *gcc
        arch: *arch
      specs:
      - !cat 
        - py-matplotlib
        - !spec {depends: *py-numpy}
        - !spec {depends: *ompi4}
        - !spec {depends: *git}
        - !spec {depends: *py-scm}
  # gcc R
  - - !compiled
      compiler: *gcc
      arch: *arch
      specs:
      - !cat &r
        - r@4.3.0
        - build_system=autotools +external-lapack
        # constrain cmake to reuse tbb and mkl below
        - !spec {depends: cmake ^curl+libidn2}
        - !spec {depends: *ompi4}
        - !spec {depends: *openjdk}
        # something triggers a change to mkl
        - !spec {depends: *mkl}
        - !spec {depends: *tbb}
    - !loop_depends
      base: *r
      specs:
      - r-dplyr@1.1.2
      - r-ggplot2@3.4.2
      - r-bit@4.0.5
      - r-bit64@4.0.5
      - r-withr@2.5.0
      - r-assertthat@0.2.1 
      - r-magrittr@2.0.3
      - r-glue@1.6.2
      - r-rlang@1.1.0
      - r-r6@2.5.1
      - r-tidyselect@1.2.0
      - r-rcpp@1.0.10
      - r-cli@3.6.1
      - r-lifecycle@1.0.3
      - r-vctrs@0.6.2
      - r-purrr@1.0.1
      - r-fs@1.6.2
      - r-lobstr@1.1.2
      - r-cpp11@0.4.3
    # bioconductor
    - !loop_depends
      base: *r
      specs:
      # core items
      - r-biocmanager
      - r-biocinstaller
      # requested
      - r-biostrings
      - r-genomicranges
      # dependents
      # dev: note that we can hide implicits so these might not strictly be necessary
      - r-biostrings
      - r-biocgenerics
      - r-genomeinfodb
      - r-genomeinfodbdata
      - r-rcurl
      - r-bitops
      - r-iranges
      - r-s4vectors
      - r-xvector
      - r-zlibbioc
      - r-genomicranges
      - r-biocgenerics
      - r-genomeinfodb
      - r-genomeinfodbdata
      - r-rcurl
      - r-bitops
      - r-iranges
      - r-s4vectors
      - r-xvector
      - r-zlibbioc
  # depends on gcc, openmpi, gcc-exclusive
  - - !loop_depends
      base: *ompi4
      specs: 
      - !cat &py-mpi4py 
        - py-mpi4py@3.1.4
        - !spec {depends: *py-cython-29}
        - !spec {depends: *python}
      - !cat &parmetis 
        - parmetis@4.0.3
        - !spec {depends: *metis}
  # depends on gcc, openmpi, mkl, cuda
  - - !builder
      # note thinking about changing target to one with AVX2_256 because GROMACS
      #   produces a warning: 
      #     Compiled SIMD is AVX_512, but AVX2_256 might be faster (see log).
      #   however we have 2 FMA units on our processors so 512 should be faster
      suffix: !spec {depends: *ompi4}
      tail: *mkl
      specs:
      - !cat
        - &gmx-base gromacs@2023.3
        - &gmx-variants build_system=cmake build_type=Release
          ~cp2k +cuda cuda_arch=89 ~cycle_subcounters
          ~double +hwloc ~intel_provided_gcc ~mdrun_only
          +mpi ~nosuffix ~opencl +openmp openmp_max_threads=none 
          ~relaxed_double_precision
          +shared ~sycl
          ~cufftmp
        - !spec {depends: *cuda}
      # alternate copy of gromacs with cuFFTMp
      - !cat
        - &gmx-base gromacs@2023.3
        - &gmx-variants build_system=cmake build_type=Release
          ~cp2k +cuda cuda_arch=89 ~cycle_subcounters
          ~double +hwloc ~intel_provided_gcc ~mdrun_only
          +mpi ~nosuffix ~opencl +openmp openmp_max_threads=none 
          ~relaxed_double_precision
          +shared ~sycl
          +cufftmp
        - !spec {depends: *nvhpc}
        - !spec {depends: *cuda}
  # depends on gcc, openmpi, mkl, cuda
  - - !builder
      suffix: !spec {depends: *ompi4}
      tail: *mkl
      specs:
      - !cat
        - &lammps-base lammps@20230802 
        - &lammps-variant
          +asphere +body  build_system=cmake build_type=RelWithDebInfo
          +class2 +colloid +compress +coreshell
          +cuda  cuda_arch=89 +cuda_mps 
          +dipole +exceptions ~ffmpeg +granular ~ipo ~jpeg 
          +kim ~kokkos 
          +kspace fftw_precision=double
          lammps_sizes=smallbig 
          +lib +manybody +mc +misc +molecule +mpi +mpiio +openmp
          +opt +peri +png +poems +python +qeq +replica +rigid ~rocm +shock +srd +voronoi
          +atc +h5md +meam +netcdf +phonon +qtb +reaxff
          +extra-pair
        - !spec {depends: *python}
        - !spec {depends: *py-mpi4py}
        - !spec {depends: *netcdf-c}
        # dev: errors on kokkos 
        #   basically cmake says it cannot get the architecture, even though the value is set properly
        #   - !spec {depends: kokkos +wrapper +cuda cuda_arch=89}
        - !spec {depends: *cuda}
        - !spec {depends: *openblas}
      # building generic cast of characters
      - &hypre hypre@2.20.0 +openmp
      - !cat &superlu-dist
        - superlu-dist@8.1.2
        - !spec {depends: *metis}
        - !spec {depends: *parmetis}
      - !cat &petsc 
        - petsc@3.20.1
        - !spec {depends: *hdf5}
        - !spec {depends: *hypre}
        - !spec {depends: *metis}
        - !spec {depends: *parmetis}
        - !spec {depends: *superlu-dist}
        - !spec {depends: *python}
      - !cat &boost
        - boost@1.83.0
          +atomic +chrono ~clanglibcpp ~container ~context ~contract ~coroutine +date_time
          ~debug +exception ~fiber +filesystem +graph ~graph_parallel ~icu +iostreams 
          +json +locale +log +math +mpi +multithreaded ~nowide +numpy +pic
          +program_options +python +random +regex +serialization +shared +signals 
          ~singlethreaded ~stacktrace +system ~taggedlayout +test +thread +timer 
          ~type_erasure ~versionedlayout +wave 
        - !spec {depends: *py-numpy}
      - !cat
        - arrow@15.0.1
          ~brotli ~compute ~cuda ~gandiva ~glog ~hdfs +ipc ~ipo ~jemalloc +lz4 ~orc +parquet
          +python +shared ~snappy ~tensorflow +zlib +zstd
          +csv
        - !spec {depends: *boost}
        - !spec {depends: *py-numpy}
      - !cat
        - arrow@10.0.1
        - !spec {depends: *boost}
        - !spec {depends: *py-numpy}
  # requirements for aLENS for div206
  - - !loop_depends
      base: *ompi4
      specs: 
      # dev: aLENS specifically requests trilinos@12.18 but I get errors
      - !cat
        - vtk@9.2.6 
        - !spec {depends: *netcdf-c}
        # note that vtk depends on netcdf-cxx@4 and we have netcdf-cxx4 instead
        - !spec {depends: *eigen}
      - !cat &trilinos
        - trilinos@14.4.0
          +zoltan +zoltan2
        - !spec {depends: *mkl}
  # namd on gcc
  #! - - !compiled
  #!     compiler: *gcc
  #!     specs:
  #!     - !cat
  #!       # note that this has been a major struggle
  #!       - namd@2.14 
  #!       - !cat &namd-support 
  #!         - ~single_node_gpu +cuda cuda_arch=89 interface=tcl
  #!         - !spec {depends: *mkl}
  #!         - !spec {depends: *ompi4}
  #!         - !spec {depends: *cuda}
  #!         # it is extremely counterintuitive but namd +cuda does not use charmpp +cuda
  #!         #   according to some paper I found
  #!         # iteratively built this recipe with charmpp last
  #!         # dev: we do not have pmix.h and backend=ucx pmi=slurmpmi2 says you cannot use
  #!         #   ompi and you need an impi instead. perhaps mvapich2? in the meantime, doing smp
  #!         - !spec 
  #!           depends: !cat
  #!           # ucx requires pmi but pmi=slurmpmi2 requires a non-openmpi MPI
  #!           #   so I give up on ucx and try verbs
  #!           # ucx is probably best, see
  #!           #   https://www.ks.uiuc.edu/Research/namd/wiki/index.cgi?NamdOnInfiniBand
  #!           - charmpp +smp backend=verbs
  #!           # - !spec {depends: *ucx-external}
  # duplicate standard programs on intel
  - - !compiled
      compiler: *intel
      arch: *arch
      specs: *compiled-specs 
  # intel compiled exclusives
  - - !compiled
      compiler: *intel
      arch: *arch
      specs:
      - *openblas
  # intel python
  - - !compiled
      compiler: *intel
      arch: *arch
      specs:
      # due to a compiler bug in oneapi explained in the Python Spack package,
      #   we have to hang out in 2019
      - &python-intel python@3.8.18
  - - !loop_depends
      base: !cat
      - *python-intel
      - !spec 
        compiler: *intel
        arch: *arch
      specs: &python-support-intel
      # suport for py-cython and hence py-numpy and scipy below is standardized
      #   around a single py-pip, py-wheel, and py-setuptools at maximum version
      #   for Python 3.8 which is the highest we can go with %oneapi
      - &py-setuptools-intel py-setuptools@63.4.3
      - &py-wheel-intel py-wheel@0.37.1
      - &py-pip-intel py-pip@23.1.2
      - !cat 
        - py-packaging@23.1
        - !spec {depends: *py-wheel-intel}
      - !cat &py-cython-29-intel
        - py-cython@0.29.36
        - !spec {depends: *py-setuptools-intel}
        - !spec {depends: *py-wheel-intel}
        - !spec {depends: *py-pip-intel}
        - !spec {depends: *python-intel}
  - - !compiled
      compiler: *intel
      arch: *arch
      specs:
      - !cat &py-numpy-intel
        - py-numpy@1.24.4
        - !spec {depends: *mkl}
        - !spec {depends: *py-cython-29-intel}
        - !spec {depends: *intel-mpi-base}
      - !cat &py-scipy-intel
        - py-scipy@1.10.1
        - !spec {depends: *py-cython-29-intel}
  - - !loop_depends
      base: !cat
      - *python-intel
      - !spec 
        compiler: *intel
        arch: *arch
      specs:
      - !cat 
        - py-matplotlib
        - !spec {depends: *py-numpy-intel}
        - !spec {depends: *intel-mpi-base}
        - !spec {depends: *git}
        - !spec {depends: *py-scm}
  # depends on intel, intel-mpi
  - - !loop_depends
      base: *intel-mpi
      specs: *mpi-general 
  # namd on gcc
  #! icx is not supported in this version. also I get errors with MPI and the docs indicate that
  #!   you need some kind of icc architecture for openmpi anyway
  #! - - !loop_depends
  #!     base: *intel-mpi
  #!     specs: 
  #!     - !cat
  #!       - namd@2.14 
  #!       - !cat &namd-support 
  #!         - +single_node_gpu +cuda cuda_arch=89 interface=tcl
  #!         - !spec &charmpp-dep
  #!           depends: !cat
  #!           - charmpp
  #!           - +cuda +omp ~papi 
  #!             backend=multicore
  #!             +production ~pthreads 
  #!             +shared +smp ~syncft ~tcp ~tracing 
  #!             build-target=LIBS build_system=generic
  #!           - !spec {depends: *cuda}
  #!         - !spec {depends: *cuda}
  #!         - !spec {depends: *mkl}
  #!     - !cat
  #!       - namd@3.0b3 
  #!       - *namd-support
  #!       - *charmpp-dep
  # depends on intel, intel-mpi, exclusives
  - - !loop_depends
      base: *intel-mpi
      specs: 
      - &hypre-intel !cat
        - hypre@2.20.0 +openmp +int64
        - !spec {depends: *mkl}
